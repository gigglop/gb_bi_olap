### Подготовка окружения
Сначала требуется подготовить окружение:
* развернуть PostgreSQL, прописать настройки подключения и названия БД в переменных 
* создать соответствующие базы данных (названия должны соответствовать переменным).

Для корректной работы используется файл .env, содержащий параметры подключения к БД. Он должен располагаться в основной 
директории проекта. Он должен содержать следующие переменные:
- параметры БД stage:
  - для подключения к БД 
    - `STAGE_POSTGRES_HOST`
    - `STAGE_POSTGRES_PORT`
    - `STAGE_POSTGRES_USER`
    - `STAGE_POSTGRES_PASS`
    - `STAGE_POSTGRES_DB`
  - таблицы
    - `STAGE_POSTGRES_DB_META_TABLE` - название таблицы, которая будет содержать метаданные о результатах ETL CSV файлов в stage
    - `STAGE_SALES_SOURCE_TABLE` - название таблицы, в которую загружаются данные из CSV файлов в stage
- параметры БД ods:
  - для подключения к БД
    - `ODS_POSTGRES_HOST`
    - `ODS_POSTGRES_PORT`
    - `ODS_POSTGRES_USER`
    - `ODS_POSTGRES_PASS`
    - `ODS_POSTGRES_DB`
- 

Ниже пример наполнения `.env`:
```
STAGE_POSTGRES_HOST=localhost
STAGE_POSTGRES_PORT=5432
STAGE_POSTGRES_USER=postgres
STAGE_POSTGRES_PASS=postgres
STAGE_POSTGRES_DB=stage
STAGE_POSTGRES_DB_META_TABLE=meta_info
STAGE_SALES_SOURCE_TABLE=source_sales
ODS_POSTGRES_HOST=localhost
ODS_POSTGRES_PORT=5432
ODS_POSTGRES_USER=postgres
ODS_POSTGRES_PASS=postgres
ODS_POSTGRES_DB=ods
```

Помимо этого в директории `stage` должна присутвовать директория `source_data` с поддиректориями: 
- `source/new` - в нее должны складываться CSV файлы продаж (источник: https://www.kaggle.com/datasets/knightbearr/sales-product-data), для которых требуется выполнить ETL
- `source/processed` - в нее будут помещаться CSV файлы после успешного процесса ETL 
- `source/error` - в нее будут помещаться CSV файлы, в которых возникли критические ошибки во время ETL (критические в том смысле, что ETL не мог быть продолжен после ошибки) 

  
Для выполнения скриптов требуeтся `python version 3.10` с пакетами:
* `psycopg2` 
* `python-dotenv`
* `os`
* `csv`
* `datetime`

и командная оболочка `bash`.

### ETL из csv в stage
#### Запуск
1. Выполнить `stage/db_structure.py` для создания таблицы с метаданными и таблицы, в которую будут загружаться данные из CSV файлов
2. Загрузить файлы CSV источника в директорию `stage/source/new`
3. Запустить процесс ETL из csv файлов в БД stage, выполнив python-скрипт `stage/etl.py`. В директории есть также файл cron.sh для планировщика crontab. Пример строки для планировщика:
```
10 23 * * * bash ~/gb_bi/stage/cron.sh
```
#### Порядок
ETL запускается для каждого csv-файла отдельно. Например, если в процессе ETL для одного файла возникли ошибки, это никак не повлияет на запуск ETL для других файлов. 

#### Лог
Результат и лог выполнения ETL можно посмотреть в таблице с метаинформацией. Название таблицы опеределяется переменной в `.env`

#### Data quality
Во время ETL данные выгружаются практически в неизменном виде. 
Данные из источника проходят минимальную проверку:
  * строка из CSV должна быть уникальна в рамках одного ETL, 
  * игнорируются строки с пустыми значениями, 
  * игнорируются строки с заголовками.
Все факты игнорирования строки из CSV фиксируются в логе ETL с указанием номера в источнике.
  

### ETL из stage в ods
#### Запуск
1. Выполнить `ods/db_structure.py` для создания схемы таблиц. Названия таблиц и их структуру определены в этом же файле.
2. Запустить процесс ETL, выполнив python-скрипт `ods/etl.py`. В директории есть также файл cron.sh для планировщика crontab. Пример строки для планировщика:
```
10 23 * * * bash ~/gb_bi/ods/cron.sh
```

#### Порядок
Сначала выполняется загрузка новых записей (определяется по временной метке `created_at` из таблицы источника).
После происходит попытка обновление существующих записей (определяется по временной метке `updated_at` из таблицы источника).
Ошибки, возникшие на этапе загрузки новых записей не повлияют на этап обновления записей. И наоборот.
После каждого этапа происходит commit изменений.

#### Лог
Результат и лог выполнения ETL можно посмотреть в таблице с метаинформацией. Название таблицы определено в файле `db_structure.py`.
Логируются результаты выполнения каждого этапа с детализацией и с указанием временных меток.

#### Data quality
Вставляются только уникальные записи из источника.

### BI
Для создания OLAP-структур отдельно создано представление `Orders` в БД ods. 
В качестве инструмента визуализации можно использовать любой, имеющий интеграцию с PostgreSQL.
Например, Apache Superset.